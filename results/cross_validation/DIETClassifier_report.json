{
  "age": {
    "precision": 0.7586206896551724,
    "recall": 0.8461538461538461,
    "f1-score": 0.8,
    "support": 26,
    "confused_with": {
      "activity_level": 2
    }
  },
  "activity_level": {
    "precision": 0.6153846153846154,
    "recall": 0.7619047619047619,
    "f1-score": 0.6808510638297872,
    "support": 21,
    "confused_with": {}
  },
  "height": {
    "precision": 0.9629629629629629,
    "recall": 0.7878787878787878,
    "f1-score": 0.8666666666666665,
    "support": 33,
    "confused_with": {
      "age": 3,
      "activity_level": 3
    }
  },
  "diet": {
    "precision": 0.8846153846153846,
    "recall": 0.7540983606557377,
    "f1-score": 0.8141592920353982,
    "support": 61,
    "confused_with": {}
  },
  "weight": {
    "precision": 0.8333333333333334,
    "recall": 0.8333333333333334,
    "f1-score": 0.8333333333333334,
    "support": 24,
    "confused_with": {
      "age": 2,
      "activity_level": 2
    }
  },
  "gender": {
    "precision": 0.9333333333333333,
    "recall": 0.9333333333333333,
    "f1-score": 0.9333333333333333,
    "support": 15,
    "confused_with": {}
  },
  "allergen": {
    "precision": 0.9666666666666667,
    "recall": 0.6904761904761905,
    "f1-score": 0.8055555555555556,
    "support": 42,
    "confused_with": {
      "activity_level": 2
    }
  },
  "micro avg": {
    "precision": 0.8522167487684729,
    "recall": 0.7792792792792793,
    "f1-score": 0.8141176470588236,
    "support": 222
  },
  "macro avg": {
    "precision": 0.8507024265644956,
    "recall": 0.8010255162479988,
    "f1-score": 0.8191284635362963,
    "support": 222
  },
  "weighted avg": {
    "precision": 0.8693086986190434,
    "recall": 0.7792792792792793,
    "f1-score": 0.8161933445401719,
    "support": 222
  },
  "accuracy": 0.9727427597955707
}