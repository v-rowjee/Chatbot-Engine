{
  "affirm": {
    "precision": 0.7647058823529411,
    "recall": 0.8666666666666667,
    "f1-score": 0.8125,
    "support": 15,
    "confused_with": {
      "thanks": 1,
      "deny": 1
    }
  },
  "greet": {
    "precision": 0.9,
    "recall": 0.6923076923076923,
    "f1-score": 0.7826086956521738,
    "support": 13,
    "confused_with": {
      "affirm": 3,
      "deny": 1
    }
  },
  "inform_allergens": {
    "precision": 1.0,
    "recall": 0.6666666666666666,
    "f1-score": 0.8,
    "support": 9,
    "confused_with": {
      "thanks": 1,
      "inform_diet": 1
    }
  },
  "ask_diet_plan": {
    "precision": 0.6923076923076923,
    "recall": 0.9,
    "f1-score": 0.7826086956521738,
    "support": 10,
    "confused_with": {
      "inform_diet": 1
    }
  },
  "thanks": {
    "precision": 0.8,
    "recall": 1.0,
    "f1-score": 0.888888888888889,
    "support": 16,
    "confused_with": {}
  },
  "deny": {
    "precision": 0.8181818181818182,
    "recall": 0.6428571428571429,
    "f1-score": 0.7200000000000001,
    "support": 14,
    "confused_with": {
      "thanks": 2,
      "inform_diet": 1
    }
  },
  "inform_diet": {
    "precision": 0.8,
    "recall": 0.8,
    "f1-score": 0.8000000000000002,
    "support": 15,
    "confused_with": {
      "ask_diet_plan": 3
    }
  },
  "accuracy": 0.8043478260869565,
  "macro avg": {
    "precision": 0.8250279132632073,
    "recall": 0.7954997383568811,
    "f1-score": 0.7980866114561767,
    "support": 92
  },
  "weighted avg": {
    "precision": 0.8190022892708314,
    "recall": 0.8043478260869565,
    "f1-score": 0.8009752415458936,
    "support": 92
  },
  "micro avg": {
    "precision": 0.8043478260869565,
    "recall": 0.8043478260869565,
    "f1-score": 0.8043478260869565,
    "support": 92
  }
}